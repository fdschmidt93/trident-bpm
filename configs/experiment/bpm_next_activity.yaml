# @package _global_

defaults:
  - default
  - /dataspec@datamodule.train.activity: bpm_activity
  - /dataspec@datamodule.val.val_activity: bpm_activity
  - /dataspec@datamodule.val.test_activity: bpm_activity
  - /dataspec@datamodule.test.val_activity: bpm_activity
  - /dataspec@datamodule.test.test_activity: bpm_activity
  - override /callbacks: null

run:
  task: activity
  lang: en
  seed: 42
  train_batch_size: 2
  val_test_batch_size: 16
  pretrained_model_name_or_path: "meta-llama/Meta-Llama-3-8B-Instruct"
  # pretrained_model_name_or_path: "mistralai/Mistral-7B-Instruct-v0.2"
  # pretrained_model_name_or_path: "openai-community/gpt2"
  tokenizer: 
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
    padding_side: left

trainer:
  max_epochs: 3
  accumulate_grad_batches: 16
  num_sanity_val_steps: 0
  precision: bf16
  check_val_every_n_epoch: 1
  val_check_interval: 0.33

datamodule:
  train:
    activity:
      dataset:
        split: train
      dataloader:
        shuffle: true
        batch_size: ${run.train_batch_size}
  val:
    val_activity:
      dataset:
        split: val
      dataloader:
        batch_size: ${run.val_test_batch_size}
    test_activity:
      dataset:
        split: test
      dataloader:
        batch_size: ${run.val_test_batch_size}
  test:
    val_activity:
      dataset:
        split: val
      dataloader:
        batch_size: ${run.val_test_batch_size}
    test_activity:
      dataset:
        split: test
      dataloader:
        batch_size: ${run.val_test_batch_size}

module:
  _target_: src.bpm.modules.LLMForSequenceClassification
  label_tokens:
    - "0"
    - "A"
    - "B"
    - "C"
    - "D"
    - "E"
    - "F"
    - "G"
    - "H"
    - "I"
    - "J"
    - "K"
    - "L"
    - "M"
    - "N"
    - "O"
    - "P"
    - "Q"
    - "R"
    - "S"
    - "T"
    - "U"
    - "V"
    - "W"
    - "X"
    - "Y"
    - "Z"
  tokenizer: ${run.tokenizer}
  optimizer:
    # _target_: bitsandbytes.optim.AdamW8bit
    _target_: torch.optim.AdamW
    lr: 1e-5
  scheduler:
    num_warmup_steps: 0
  # model:
  #   _target_: transformers.AutoModelForCausalLM.from_pretrained
  #   pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
  # model:
  #   _target_: transformers.AutoModelForCausalLM.from_pretrained
  #   pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
  #   device_map: "auto"
  #   quantization_config: 
  #     _target_: transformers.BitsAndBytesConfig
  #     load_in_4bit: True
  #     bnb_4bit_quant_type: "nf4"
  #     bnb_4bit_use_double_quant: True
  #     bnb_4bit_compute_dtype:
  #       _target_: src.bpm.utils.get_torch_dtype
  #       type_: bfloat16
  model:
    _target_: peft.get_peft_model
    model:
      _target_: transformers.AutoModelForCausalLM.from_pretrained
      pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
      device_map: "auto"
      quantization_config: 
        _target_: transformers.BitsAndBytesConfig
        load_in_4bit: True
        bnb_4bit_quant_type: "nf4"
        bnb_4bit_use_double_quant: True
        bnb_4bit_compute_dtype:
          _target_: src.bpm.utils.get_torch_dtype
          type_: bfloat16
    peft_config:
      _target_: peft.LoraConfig
      r: 64
      lora_alpha: 128
      target_modules: all-linear
      lora_dropout: 0.05 
      bias: "none" 
      task_type: "CAUSAL_LM"
      # init_lora_weights: loftq
      # loftq_config:
      #   _target_: peft.LoftQConfig
      #   loftq_bits: 4

logger:
  wandb:
    name: "lang=${run.lang}_model=${run.pretrained_model_name_or_path}_epochs=${trainer.max_epochs}_bs=${_log_vars.train_batch_size}_lr=${module.optimizer.lr}_scheduler=${module.scheduler.num_warmup_steps}_seed=${run.seed}"
    tags:
      - "${run.pretrained_model_name_or_path}"
      - "bs=${_log_vars.train_batch_size}"
      - "lr=${module.optimizer.lr}"
      - "scheduler=${module.scheduler.num_warmup_steps}"
    project: ${run.task}

callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
